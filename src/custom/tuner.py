import pandas as pd
from mlflow.entities import Experiment
from rich.panel import Panel
from sklearn.model_selection import cross_val_predict, cross_val_score

from src.core.gridsearch import MLflowGridSearchCV
from src.core.tuner import ModelTuner
from src.custom.plots import plot_roc_curves_after_cv
from src.custom.richer import console, logger
from src.models.classifier import ClassifierModel
from src.models.params import Params
from src.models.regressor import RegressorModel


class CustomModelTuner(ModelTuner):
    """
    CustomModelTuner is a class that extends the ModelTuner to provide
    specific tuning capabilities for machine learning models. It utilizes
    parameters defined in the Params class and integrates with MLflow
    for experiment tracking.

    This class provides methods for creating a grid search cross-validation
    object and performing hyperparameter tuning while comparing the tuned
    model's performance with a baseline model.

    Attributes
    ----------
    tuner_params : Params
        Parameters for configuring the tuning process, including the pipeline,
        parameter grid, scoring metric, and other settings.
    _experiment : Experiment
        An MLflow Experiment object for logging metrics and tracking experiments.

    Methods
    -------
    create_search_cv() -> MLflowGridSearchCV
        Creates an instance of MLflowGridSearchCV for hyperparameter tuning.
    tune(X_train: pd.DataFrame, y_train: pd.Series, baseline: ClassifierModel | RegressorModel) -> ClassifierModel | RegressorModel
        Tunes a model using hyperparameter optimization and compares it with a baseline model.

    Notes
    -----
    - The `create_search_cv` method initializes a grid search cross-validation object
      that integrates with MLflow for experiment tracking.
    - The `tune` method evaluates the tuned model's performance and compares it with
      the baseline model, returning the better-performing model.
    """

    def __init__(self, tuner_params: Params, experiment: Experiment):
        self.tuner_params = tuner_params
        self._experiment = experiment

    def create_search_cv(self) -> MLflowGridSearchCV:
        """
        Creates an instance of MLflowGridSearchCV for hyperparameter tuning.

        This method initializes a grid search cross-validation object that integrates
        with MLflow for experiment tracking. It uses the parameters defined in the
        `tuner_params` attribute to configure the grid search.

        Returns
        -------
        MLflowGridSearchCV
            An instance of MLflowGridSearchCV configured with the specified parameters.

        Notes
        -----
        - The `param_grid` is generated by calling `model_dump()` on each parameter
          grid defined in `tuner_params.param_grid`.
        - The returned object is ready to perform grid search with MLflow logging enabled
          if `tuner_params.enable_mlflow` is set to True.
        """
        return MLflowGridSearchCV(
            experiment=self._experiment,
            enable_mlflow=self.tuner_params.enable_mlflow,
            estimator=self.tuner_params.pipeline,
            param_grid=[param.model_dump() for param in self.tuner_params.param_grid],
            scoring=self.tuner_params.scoring,
            cv=self.tuner_params.inner_cv,
            n_jobs=self.tuner_params.n_jobs,
            verbose=self.tuner_params.verbose,
        )

    def tune(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        baseline: ClassifierModel | RegressorModel,
    ) -> ClassifierModel | RegressorModel:
        """
        Tunes a model using hyperparameter optimization and compares it with a baseline model.

        This method performs hyperparameter tuning using the `create_search_cv` method, evaluates
        the tuned model's performance, and compares it with the provided baseline model. If the
        baseline model performs better, it is returned; otherwise, the tuned model is returned.

        Parameters
        ----------
        X_train : pd.DataFrame
            The feature matrix for training the model.
        y_train : pd.Series
            The target variable for training the model.
        baseline : ClassifierModel | RegressorModel
            The baseline model to compare against during the tuning process.

        Returns
        -------
        ClassifierModel | RegressorModel
            The better-performing model, either the baseline or the tuned model.

        Notes
        -----
        - Logs the best parameters, score, and estimator details of the tuned model.
        - If the baseline model's score is greater than or equal to the tuned model's score,
          the baseline model is returned.
        - The tuned model is wrapped in a `ClassifierModel` or `RegressorModel` object before returning.
        """
        console.print(
            Panel(
                "Hypertunning using Non Nested CV",
                title="GridSearchCV",
                style="warning",
            )
        )

        gscv = self.create_search_cv()
        gscv.fit(X_train, y_train)
        logger.info(f"Best params: {gscv.best_params_}")
        logger.info(f"Best roc auc score: {gscv.best_score_}")
        logger.info(f"Best estimator: {gscv.best_estimator_}")
        logger.info(
            f"Best estimator name: {gscv.best_estimator_._final_estimator.estimator.__class__.__name__}"
        )
        logger.info(
            f"Best estimator params: {gscv.best_estimator_._final_estimator.get_params()}"
        )
        if baseline.score >= gscv.best_score_:
            logger.info(
                f"Baseline model score: {baseline.score} is better than tuned model score: {gscv.best_score_}"
            )
            return baseline
        else:
            logger.info(
                f"Tuned model score: {gscv.best_score_} is better than baseline model score: {baseline.score}"
            )
            return ClassifierModel(
                name=gscv.best_estimator_._final_estimator.__class__.__name__,
                model=gscv.best_estimator_,
                params=gscv.best_params_,
                score=gscv.best_score_,
            )

    def nested_cv(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
    ) -> None:
        console.print(
            Panel(
                "Hypertunning using Nested CV",
                title="NestedCV",
                style="warning",
            )
        )
        X = pd.concat([X_train, X_test])
        y = pd.concat([y_train, y_test])

        gscv = self.create_search_cv()
        y_pred_proba = cross_val_predict(
            gscv, X, y, cv=self.tuner_params.outer_cv, method="predict_proba"
        )
        nested_scores = cross_val_score(
            gscv,
            X,
            y,
            cv=self.tuner_params.outer_cv,
            scoring=self.tuner_params.scoring,
        )
        console.log(
            f"Mean Outer CV score: {nested_scores.mean():.4f} (std: {nested_scores.std():.4f})",
            style="info",
        )
        _ = plot_roc_curves_after_cv(
            y_true=y,
            y_pred_proba=y_pred_proba,
            target_names=["Class 0", "Class 1", "Class 3"],
        )
